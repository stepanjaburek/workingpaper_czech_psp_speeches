# -*- coding: utf-8 -*-
"""CzechParlPaper_PolDEBATE_V2_September.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Yx9fWzeNEx-VGVDEL5GZLrh4N1I_EZnl

<a href="https://colab.research.google.com/github/stepanjaburek/workingpaper_czech_psp_speeches/blob/main/streamline_translation_sentiment.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# Setup
"""

!pip install transformers sentencepiece sacremoses torch tqdm

import pandas as pd
from tqdm.notebook import tqdm
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from transformers import pipeline
from transformers import MarianMTModel, MarianTokenizer
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import f1_score
from sklearn.metrics import f1_score, accuracy_score, balanced_accuracy_score, precision_score, recall_score, classification_report, matthews_corrcoef
import matplotlib.pyplot as plt
import seaborn as sns

"""# **Sentiment classification using the Political DEBATE model by Burnham et al. (2024)**

# Setup
"""

def analyze_sentiments(df, classifier, classes, hypothesis, batch_size=16):
    results = []
    for i in tqdm(range(0, len(df), batch_size)):
        batch_output = classifier(
            df['translated_context_full_opus'][i:i + batch_size].tolist(),
            classes,
            hypothesis_template=hypothesis,
            multi_label=False,
            batch_size=batch_size
        )

        for item in batch_output:
            results.append({
                'label': item['labels'][0],
                'score': item['scores'][0],
                **{f'{label_2}_score': score for label_2, score in zip(item['labels'], item['scores'])}
            })

    return pd.DataFrame(results)

"""# Model specification and labeling - Left"""

model_name = "mlburnham/Political_DEBATE_large_v1.0"

#hypothesis_template =  "The emotional valence of this text towards the political left is {}"
#hypothesis_template =  "The author expresses {} sentiment towards the political left"
hypothesis_template =    "The emotional valence expressed towards the political left is {}"

classes = [ "negative", "neutral", "positive"]

device = 0 if torch.cuda.is_available() else -1
classifier = pipeline("zero-shot-classification",
                     model=model_name,
                     device=device)

df = pd.read_csv("/content/left_translated_opus_v2.csv")
results = analyze_sentiments(df, classifier, classes, hypothesis_template)

pd.concat([df, results], axis=1).to_csv('debate_sentiment_left_opus_v2.csv', index=False)


print("\nSentiment Distribution:")
print(results['label'].value_counts())

"""# Model specification and labeling - Right"""

model_name = "mlburnham/Political_DEBATE_large_v1.0"
hypothesis_template =   "The emotional valence expressed towards the political right is {}"
classes = [ "negative", "neutral", "positive"]

device = 0 if torch.cuda.is_available() else -1
classifier = pipeline("zero-shot-classification",
                     model=model_name,
                     device=device)

df = pd.read_csv("/content/right_translated_opus_v2.csv")
results = analyze_sentiments(df, classifier, classes, hypothesis_template)


pd.concat([df, results], axis=1).to_csv('debate_sentiment_right_opus_v2.csv', index=False)


print("\nSentiment Distribution:")
print(results['label'].value_counts())

df1 = pd.read_csv("/content/debate_sentiment_left.csv")
df2 = pd.read_csv("/content/debate_sentiment_right.csv")
result = pd.concat([df1, df2], ignore_index=True)
actual = result['human_sentiment']
predicted = result['label']
classes = ['negative', 'neutral', 'positive'  ]
cm = confusion_matrix(actual, predicted, labels=classes)
cm
disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=classes)
disp.plot()

test = pd.read_csv("/content/debate_sentiment_left.csv")
actual = test['human_sentiment']
predicted = test['label']


# F1 scores
f1_micro = f1_score(actual, predicted, average='micro')
f1_weighted = f1_score(actual, predicted, average='weighted')

# Accuracy
accuracy = accuracy_score(actual, predicted)

# Balanced accuracy (useful for imbalanced datasets)
balanced_accuracy = balanced_accuracy_score(actual, predicted)

# Precision and recall
precision = precision_score(actual, predicted, average='weighted')
recall = recall_score(actual, predicted, average='weighted')
mcc = matthews_corrcoef(actual, predicted)
print(f"\nMatthews Correlation Coefficient (MCC): {mcc:.3f}")

print(f"F1 Micro: {f1_micro:.3f}")
print(f"F1 Weighted: {f1_weighted:.3f}")
print(f"Accuracy: {accuracy:.3f}")
print(f"Balanced Accuracy: {balanced_accuracy:.3f}")
print(f"Precision: {precision:.3f}")
print(f"Recall: {recall:.3f}")

# For a detailed classification report
print("\nClassification Report:")
print(classification_report(actual, predicted))

"""# Model specification and labeling - Topic Modeling Left"""

!pip install gensim==4.3.3 --quiet

import os
os.kill(os.getpid(), 9)

import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import matplotlib.pyplot as plt
import seaborn as sns
from gensim.utils import simple_preprocess
from gensim.models import KeyedVectors

#df = pd.read_csv("/content/debate_topic_left_meta.csv")
#df = pd.read_csv("/content/debate_topic_right_meta.csv")

df = pd.read_csv("/content/sample_data/parlstories.csv")

combined_df = pd.concat([df, de], axis=0)

# Display the result
combined_df
df = combined_df

df

# Ensure all values are string and fill NaNs
df['text'] = df['text'].fillna('').astype(str)
texts = df['text']
corpus = texts.progress_apply(lambda x: simple_preprocess(x, deacc=False)) # finalize using gensim's simple_preprocess
print(corpus.head())

    # Create a document-term matrix
count_vectorizer = CountVectorizer(
        stop_words='english',  # Remove English stop words
        max_df=0.9,           # Ignore terms that appear in >95% of documents
        min_df=5,              # Ignore terms that appear in <2 documents
        max_features=2000      # Limit vocabulary size
    )

doc_term_matrix = count_vectorizer.fit_transform(texts)
feature_names = count_vectorizer.get_feature_names_out()

# For 4500 short texts, you might want to try a smaller number of topics
lda_model = LatentDirichletAllocation(
    n_components=8,         # Reduced from 10 to 8 (adjust based on your domain knowledge)
    random_state=42,        # For reproducibility
    learning_method='online',
    max_iter=25,
    n_jobs=-1              # Use all available cores for faster processing
)

lda_output = lda_model.fit_transform(doc_term_matrix)

# Function to display the top terms in each topic
def display_topics(model, feature_names, no_top_words):
    topic_dict = {}
    for topic_idx, topic in enumerate(model.components_):
        topic_words = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]
        topic_dict[topic_idx] = topic_words
        print(f"Topic {topic_idx}: {' '.join(topic_words)}")
    return topic_dict

# Display top 15 words in each topic (increased from 10)
topic_dict = display_topics(lda_model, feature_names, 15)

# Calculate and print perplexity score (lower is better)
perplexity = lda_model.perplexity(doc_term_matrix)
print(f"Perplexity: {perplexity}")

# Assign dominant topic to each document
topic_names = [f"Topic {i}" for i in range(lda_model.n_components)]
doc_topic_df = pd.DataFrame(lda_output, columns=topic_names)

# Get dominant topic for each document
dominant_topic = np.argmax(lda_output, axis=1)
df['dominant_topic'] = dominant_topic

# Visualization - plot the distribution of topics
plt.figure(figsize=(12, 6))
sns.countplot(x='dominant_topic', data=df)
plt.title('Distribution of Dominant Topics')
plt.xlabel('Topic Number')
plt.ylabel('Number of Documents')
plt.xticks(range(lda_model.n_components))
plt.show()

doc_topic_dist

model_name = "mlburnham/Political_DEBATE_large_v1.0"
hypothesis_template = "This text discusses the political left in terms of {}"
classes = ["economic issues",
    "social/cultural issues",
    "institutional issues",
    "historical issues",
    "foreign policy issues"]

device = 0 if torch.cuda.is_available() else -1
classifier = pipeline("zero-shot-classification",
                     model=model_name,
                     device=device)

df = pd.read_csv("/content/left_translated.csv")
results = analyze_sentiments(df, classifier, classes, hypothesis_template)


pd.concat([df, results], axis=1).to_csv('debate_topic_left.csv', index=False)


print("\nSentiment Distribution:")
print(results['label'].value_counts())