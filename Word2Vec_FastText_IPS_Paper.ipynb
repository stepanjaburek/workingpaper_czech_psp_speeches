{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "private_outputs": true,
      "authorship_tag": "ABX9TyMsjmd1JEnCpM22c7/K31Zd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stepanjaburek/workingpaper_czech_psp_speeches/blob/main/Word2Vec_FastText_IPS_Paper.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word embeddings"
      ],
      "metadata": {
        "id": "M_fYIsWA5CR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "p3fCeVmu5G6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.24.4 --quiet # somehow new numpy doesnt work well with gensim\n",
        "!pip install gensim==4.3.3 --quiet\n",
        "\n",
        "import os\n",
        "os.kill(os.getpid(), 9)"
      ],
      "metadata": {
        "id": "1mUAMqOQxlDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmEUNUbYxCKv"
      },
      "outputs": [],
      "source": [
        "!pip install stanza --quiet\n",
        "import stanza\n",
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.models import FastText\n",
        "from gensim.models import KeyedVectors\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv('/content/word2vec_left.csv')\n",
        "df = pd.read_csv('/content/word2vec_right.csv')"
      ],
      "metadata": {
        "id": "2kV3omCh5TzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup lemma"
      ],
      "metadata": {
        "id": "IF9R6eFN5PFv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stanza.download(\"cs\")  # get the Czech model\n",
        "nlp = stanza.Pipeline(\"cs\", processors=\"tokenize,lemma\")"
      ],
      "metadata": {
        "id": "uH58mJgVTu9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"levicový poslanec zvolil levicovou vládu a levicová maláčová..babiš babiše babišem\") # test lemma setup\n",
        "lemmas = [word.lemma for sent in doc.sentences for word in sent.words]\n",
        "print(lemmas)"
      ],
      "metadata": {
        "id": "aCFEH8JVUWlz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the lemmatization function\n",
        "def lemmatize_text(text):\n",
        "    text = str(text)\n",
        "    doc = nlp(text)  # apply Stanza NLP\n",
        "    lemmas = [word.lemma for sent in doc.sentences for word in sent.words if word.upos != 'PUNCT']     # Get lemmatized tokens (exclude punctuation and spaces)\n",
        "    return \" \".join(lemmas)     # Return the lemmatized text as a string\n",
        "# Claude helping here"
      ],
      "metadata": {
        "id": "LJuHvho7WpZ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text'] = df['text'].fillna('').astype(str) # be sure with NAs, shouldnt be any though\n",
        "df['text_lemma'] = df['text'].progress_apply(lemmatize_text) # lemmatization using stanza\n",
        "\n",
        "print(df[['text_lemma', 'text']].head()) # check it"
      ],
      "metadata": {
        "id": "z_jy3eS9-mf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Or Import already lemmatizated data instead"
      ],
      "metadata": {
        "id": "35muXKU1q2Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df = pd.read_csv('/content/lemmatized_data_left.csv')\n",
        "df = pd.read_csv('/content/lemmatized_data_right.csv')"
      ],
      "metadata": {
        "id": "Dk9vt_8j5nFk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FastText"
      ],
      "metadata": {
        "id": "G6HOEWqoq9LH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = df['text_lemma']\n",
        "corpus = texts.progress_apply(lambda x: simple_preprocess(x, deacc=False)) # finalize using gensim's simple_preprocess\n",
        "print(corpus.head())"
      ],
      "metadata": {
        "id": "_zqxfVZ2xeWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train FastText model\n",
        "model = FastText(\n",
        "    sentences=corpus,\n",
        "    vector_size=300,    # dimensions of embedding space\n",
        "    window=5,           # context window size\n",
        "    min_count=5,\n",
        "    workers=8,\n",
        "    sg=0,               # 1 = skip-gram; 0 = CBOW\n",
        "    epochs=10\n",
        ")"
      ],
      "metadata": {
        "id": "GdicaRNdfdi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the already saved word embeddings\n",
        "# Load back with memory-mapping = read-only, shared across processes.\n",
        "wv = KeyedVectors.load(\"fasttext_wordvectors_right\", mmap='r')\n",
        "vector = wv['prezident']  # Get numpy vector of a word\n",
        "wv.most_similar('prezident', topn=20)"
      ],
      "metadata": {
        "id": "mT4vJYOTsD-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('levice', topn=20)"
      ],
      "metadata": {
        "id": "DvwAoIIukC1P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get similarity between two words\n",
        "model.wv.similarity('senát', 'záchod')"
      ],
      "metadata": {
        "id": "UqihZRfXkTTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find odd word out\n",
        "model.wv.doesnt_match(['ministr', 'prezident', 'Praha', 'premiér'])"
      ],
      "metadata": {
        "id": "hlV_rhLRkO5p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word analogies (a is to b as c is to ?)\n",
        "model.wv.most_similar(positive=['levice', 'obchod'], negative=['pravice'], topn=5)\n",
        "# nefunguje moc, nebo to neumim"
      ],
      "metadata": {
        "id": "MfU69y3B6kfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# T SNE downprojection"
      ],
      "metadata": {
        "id": "tYQyUrP2rJZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install adjustText"
      ],
      "metadata": {
        "id": "E2LPZxY0kyIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.cm as cm\n",
        "from adjustText import adjust_text\n",
        "\n",
        "def plot_tsne(model, words, perplexity=35, figsize=(12, 10), colormap='hsv', adjust=True):\n",
        "    \"\"\"\n",
        "    Create a t-SNE plot for the given words using their embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    - model: FastText model\n",
        "    - words: list of words to visualize\n",
        "    - perplexity: t-SNE perplexity parameter\n",
        "    - figsize: figure size\n",
        "    - colormap: matplotlib colormap for points\n",
        "    - adjust: whether to use adjust_text to prevent label overlap\n",
        "    \"\"\"\n",
        "    # Get word vectors\n",
        "    word_vectors = np.array([model.wv[word] for word in words])\n",
        "\n",
        "    # Perform t-SNE dimensionality reduction\n",
        "    tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)\n",
        "    coordinates = tsne.fit_transform(word_vectors)\n",
        "\n",
        "    # Create a scatter plot\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Use different colors for points\n",
        "    colors = cm.get_cmap(colormap, len(words))\n",
        "\n",
        "    x = coordinates[:, 0]\n",
        "    y = coordinates[:, 1]\n",
        "\n",
        "    plt.scatter(x, y, c=range(len(words)), cmap=colors, alpha=0.7, s=100)\n",
        "\n",
        "    # Add labels for each point\n",
        "    texts = []\n",
        "    for i, word in enumerate(words):\n",
        "        texts.append(plt.text(x[i], y[i], word, fontsize=12))\n",
        "\n",
        "    if adjust:\n",
        "        # Adjust text positions to prevent overlap\n",
        "        adjust_text(texts, arrowprops=dict(arrowstyle='->', color='black'))\n",
        "\n",
        "    plt.title('t-SNE Visualization of Word Embeddings', fontsize=14)\n",
        "    plt.grid(True, linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('word_embeddings_tsne.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "# Example usage:\n",
        "# 1. Political terms\n",
        "political_terms = ['vláda', 'parlament', 'strana', 'volby', 'demokracie', 'ústava',\n",
        "                  'premiér', 'prezident', 'ministr', 'opozice', 'koalice', 'poslanec',\n",
        "                  'senátor', 'zákon', 'politika', 'justice', 'soud', 'ekonomika', 'inflace', 'rozpočet', 'daň', 'HDP', 'krize',\n",
        "                 'reforma', 'nezaměstnanost', 'mzda', 'dotace', 'investice', 'dluh',\n",
        "                 'deficit', 'banky', 'úrok', 'export', 'průmysl','levice', 'pravice', 'socialismus', 'kapitalismus', 'konzervativní',\n",
        "                     'liberální', 'komunistický', 'progresivní', 'sociální', 'solidarita',\n",
        "                     'svoboda', 'rovnost', 'trh', 'stát', 'soukromý', 'veřejný']\n",
        "\n",
        "# Run the visualization\n",
        "plot_tsne(model, political_terms)\n",
        "# Compare left/right political terms\n",
        "political_spectrum = ['levice', 'pravice', 'socialismus', 'kapitalismus', 'konzervativní',\n",
        "                     'liberální', 'komunistický', 'progresivní', 'sociální', 'solidarita',\n",
        "                     'svoboda', 'rovnost', 'trh', 'stát', 'soukromý', 'veřejný']\n",
        "plot_tsne(model, political_spectrum, perplexity=5)  # Lower perplexity for smaller set"
      ],
      "metadata": {
        "id": "xpHCw3coiNJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Position in space between two vectors"
      ],
      "metadata": {
        "id": "NAy9NAFVrNud"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_left_right_position(model, left_terms, right_terms, target_word):\n",
        "    \"\"\"Get position of a word on left-right axis\"\"\"\n",
        "    # Average left and right seed vectors\n",
        "    left_vec = np.mean([model.wv[w] for w in left_terms if w in model.wv], axis=0)\n",
        "    right_vec = np.mean([model.wv[w] for w in right_terms if w in model.wv], axis=0)\n",
        "\n",
        "    # Create left-right axis\n",
        "    lr_axis = right_vec - left_vec\n",
        "    lr_axis = lr_axis / np.linalg.norm(lr_axis)  # Normalize\n",
        "\n",
        "    # Project target word onto this axis\n",
        "    if target_word in model.wv:\n",
        "        position = np.dot(model.wv[target_word], lr_axis)\n",
        "        return position\n",
        "    else:\n",
        "        return None\n",
        "# Define seed words\n",
        "left_terms = ['levice', \"levicový\", \"levicová\"]\n",
        "right_terms = ['pravice', \"pravicový\", \"pravicová\"]\n",
        "\n",
        "# Get position of \"inflace\" (inflation)\n",
        "position = get_left_right_position(model, left_terms, right_terms, 'svoboda')\n",
        "print(f\"Position of 'inflace' on left-right axis: {position}\")\n",
        "# Positive means right-leaning, negative means left-leaning"
      ],
      "metadata": {
        "id": "pN7seb4sohZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors = model.wv\n",
        "word_vectors.save(\"fasttext_wordvectors_right\") # Store just the words + their trained embeddings.\n",
        "\n",
        "# Load back with memory-mapping = read-only, shared across processes.\n",
        "wv = KeyedVectors.load(\"fasttext_wordvectors_right\", mmap='r')\n",
        "vector = wv['prezident']  # Get numpy vector of a word\n",
        "wv.most_similar('prezident', topn=20)"
      ],
      "metadata": {
        "id": "Imco_9mMqh3E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}